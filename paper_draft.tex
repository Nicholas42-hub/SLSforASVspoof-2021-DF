%% For WWW 2026 Short Papers
%% Use sigconf, review for submission
\documentclass[sigconf, review]{acmart}

%% Rights management information. 
%% For submission, you can leave this commented out
% \setcopyright{acmlicensed}
% \copyrightyear{2026}
% \acmYear{2026}
% \acmConference[WWW '26]{Proceedings of the ACM Web Conference 2026}{April 28--May 2, 2026}{Sydney, Australia}
% \acmBooktitle{Proceedings of the ACM Web Conference 2026 (WWW '26), April 28--May 2, 2026, Sydney, Australia}
% \acmDOI{10.1145/nnnnnnn.nnnnnnn}
% \acmISBN{978-1-4503-XXXX-X/18/06}

\begin{document}

\title{Hierarchical Multi-Layer Attention with Contrastive Learning for Robust Audio Deepfake Detection}

%%
%% Authors - replace with your actual information
%%
\author{First Author}
\affiliation{%
  \institution{Your Institution}
  \city{City}
  \country{Country}
}
\email{first.author@institution.edu}

\author{Second Author}
\affiliation{%
  \institution{Your Institution}
  \city{City}
  \country{Country}
}
\email{second.author@institution.edu}

%%
%% Abstract
%%
\begin{abstract}
Recent advances in generative AI, particularly text-to-speech and voice conversion, have made synthetic speech nearly indistinguishable from genuine voices, posing critical challenges for deepfake detection. While self-supervised models like XLS-R provide rich multi-layer representations, current methods treat transformer layers independently through simple scalar weighting, failing to capture inter-layer dependencies and to generalize across domains. We introduce a unified framework integrating hierarchical multi-layer attention with contrastive learning. Unlike prior approaches, our model explicitly captures inter-layer dependencies through three-stage attention: temporal attention aggregates discriminative frame-level features within layers, intra-group attention models relationships among neighbouring layers, and inter-group attention captures interactions across layer groups, enabling systematic exploitation of complementary information. To enhance generalization, we augment the supervised objective with margin-based contrastive learning and feature diversity regularization, encouraging separation between real and fake speech representations across different domains. The attention mechanism provides interpretability by revealing which layers and temporal regions contribute most to detection decisions. Evaluated on ASVspoof 2021 DF (EER: 1.93\%) and In-the-Wild (EER: 7.04\%), our approach achieves competitive performance with state-of-the-art while demonstrating strong cross-dataset generalization. Ablation studies confirm that hierarchical inter-layer modelling and contrastive regularization provide substantial complementary benefits for robust real-world deepfake detection.
\end{abstract}

%%
%% CCS Concepts and Keywords
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010183</concept_id>
       <concept_desc>Computing methodologies~Speech recognition</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Speech recognition}

\keywords{self-supervised model, contrastive learning, hierarchical attention}

\maketitle

\section{Introduction}
Recent advances in generative artificial intelligence have enabled Text-to-Speech (TTS) and Voice Conversion (VC) systems to synthesize speech that is highly natural and nearly indistinguishable from genuine human voices~\cite{muller2022does}. While such technologies bring significant benefits in accessibility, entertainment, and human–computer interaction, they also introduce serious security and ethical risks. Audio deepfakes—synthetic voices that convincingly imitate real individuals—have already been used in social-engineering scams, including CEO fraud cases involving substantial financial losses~\cite{combei2025unmasking}. Beyond financial crime, they pose threats in misinformation, identity theft, and even judicial contexts where speech evidence may be forged~\cite{cox2023voice}. Given the increasing reliance on voice-based authentication in banking, customer service, and forensics, the development of automatic, reliable audio verification and detection systems has become an urgent research priority~\cite{combei2025unmasking}.

To counter these risks, the research community has developed increasingly sophisticated audio deepfake detection (ADD) systems. Early approaches relied on handcrafted features such as LFCC~\cite{todisco2018integrated} and CQCC~\cite{todisco2018integrated2}. While computationally efficient, these methods often fail to capture the subtle artifacts introduced by modern neural vocoders. The ASVspoof challenge series and the Audio Deepfake Detection Challenge (ADD) have played key roles in fostering the design of countermeasures to protect Automatic Speaker Verification (ASV) systems from spoofing attacks~\cite{nautsch2021asvspoof}. Recent work has shifted toward end-to-end deep learning approaches. RawNet2 processes raw waveforms directly using sinc-layer convolutions~\cite{ravanelli2018speaker,fu2022fastaudio}, while AASIST employs graph attention to model spectro-temporal relationships~\cite{jung2022aasist}. These methods have achieved strong performance on in-domain data but often struggle with generalization to unseen generation algorithms and acoustic conditions.

More recently, self-supervised learning (SSL) models pre-trained on large-scale unlabelled data have shown remarkable success in various audio tasks. Wav2vec 2.0, WavLM~\cite{chen2022wavlm}, and the multilingual XLS-R~\cite{babu2022xlsr} learn powerful representations through contrastive predictive coding and have become dominant due to their ability to capture rich contextual speech representations without labelled data. The importance of leveraging multi-layer SSL representations has been recognized in recent work—different XLS-R layers have been explored with simple downstream classifiers, achieving 4.98\% EER on ASVspoof 2021 DF. Zhang et al. proposed the SLS (Sensitive Layer Selection) module, which applies learned scalar weights to each layer independently, achieving 1.92\% EER on the same dataset~\cite{zhang2024audio}. However, these approaches do not explicitly model inter-layer dependencies, limiting their ability to capture complementary information across representation levels.

A distinctive property of large self-supervised speech models lies in their hierarchical depth, where each transformer layer progressively captures distinct aspects of the speech signal—ranging from low-level acoustic patterns to mid-level phonetic cues and high-level linguistic semantics~\cite{babu2022xlsr}. As the representation flows through the network, these features evolve in both abstraction and dependency, forming an implicit hierarchy that reflects the multi-scale nature of human speech perception. However, existing approaches that either rely on a single-layer representation or directly fuse multiple layers into a flat feature vector often overlook the inter-layer dependencies inherent in this hierarchy. Such simplifications may lead to feature homogenization, where the complementary relationships among layers are lost~\cite{xiao2025layer}. For instance, when training on data dominated by acoustic artifacts (e.g., pitch distortions), models may overemphasize shallow layers, whereas datasets emphasizing prosodic or para-linguistic cues (e.g., rhythm, stress) require deeper contextual understanding. This imbalance ultimately degrades generalization across domains.

To address the limitations of single-layer and flat fusion methods, we propose a hierarchical multi-layer attention framework that explicitly models the dependencies among transformer layers. The architecture aggregates information through a three-stage attention mechanism: temporal attention within each layer, intra-group attention across neighbouring layers, and inter-group attention across layer clusters representing low-, mid-, and high-level abstractions. This hierarchical design enables the model to dynamically allocate attention to layers that contribute most to discriminative and domain-invariant representations. To further enhance robustness, we incorporate a margin-based contrastive objective with feature diversity regularization, guiding the network to separate genuine and synthetic speech representations while preventing overfitting to domain-specific artifacts.

Our main contributions are summarised as follows:

\textbf{1) Hierarchical Multi-Layer Attention Framework.}
We introduce a novel hierarchical attention architecture that explicitly models dependencies across temporal frames, neighbouring transformer layers, and layer groups within self-supervised speech encoders. This design enables the model to leverage complementary cues across representation levels rather than treating layers independently.

\textbf{2) Contrastive Regularisation for Domain-Invariant Learning.}
To enhance robustness across unseen generation methods and recording conditions, we integrate a margin-based contrastive objective with feature diversity regularisation. This combination encourages discriminative yet domain-invariant embeddings for real and synthetic speech.

\textbf{3) Interpretability and Robust Generalisation.}
The attention mechanism provides intrinsic interpretability by revealing which temporal regions and layer hierarchies contribute most to classification decisions. Extensive experiments on ASVspoof 2021 DF and In-the-Wild datasets demonstrate competitive performance and superior cross-domain generalisation compared with prior layer-fusion approaches.

\section{Proposed Method}

\subsection{Overview}
We propose a hierarchical multi-layer attention framework that exploits the full stack of SSL encoder representations while enforcing contrastive learning. Given an input waveform $\mathbf{x} \in \mathbb{R}^s$, a pretrained XLS-R encoder produces hidden states from all transformer layers. Our model aggregates these signals through three attention stages—temporal, intra-group, and inter-group—and refines them with light residual MLP blocks. The resulting utterance embedding supports both binary classification (real vs. fake) and contrastive regularization to improve cross-domain generalization.

\subsection{SSL Feature Extraction}
The XLS-R model is a large-scale cross-lingual self-supervised speech representation framework built upon Wav2Vec 2.0. It is trained across 128 languages to capture universal acoustic and linguistic features. Given a raw speech waveform $\mathbf{x}$, a convolutional feature encoder first extracts latent representations $\mathbf{z} \in \mathbb{R}^{T \times 1024}$ at 20ms intervals using a receptive field of 25ms. These frame-level embeddings are then passed through a 24-layer transformer network to obtain contextualized hidden states $\mathbf{h}_l$, where each layer $l$ captures progressively higher-level abstractions of the speech signal. The complete stack of representations is denoted as
\begin{equation}
\mathbf{H} = [\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_L], \quad L = 24.
\end{equation}

XLS-R is trained in a self-supervised contrastive manner: a subset of latent frames is masked, and the model is optimized to identify the corresponding quantized targets among distractors using the contextualized transformer outputs. This objective encourages the model to learn generalizable speech features across languages, accents, and environments.

In this work, we fine-tune XLS-R jointly with task-specific modules as the front-end feature extractor to obtain rich, layer-wise representations for hierarchical attention and contrastive learning. Mathematically, the feature extraction process can be expressed as:
\begin{equation}
\{\mathbf{h}_l\}_{l=1}^L = \text{XLS-R}(\mathbf{x}),
\end{equation}
where $\mathbf{h}_l \in \mathbb{R}^{T \times C}$ represents the contextualized embedding at layer $l$ with $C = 1024$.

\subsection{Hierarchical Attention}
Our hierarchical attention mechanism operates in three stages, progressively aggregating information from frame-level to utterance-level representations while explicitly modeling inter-layer dependencies.

\textbf{Temporal attention (within-layer).} For each layer $l$, we summarize its temporal sequence $\mathbf{h}_l$ into a single token using a lightweight MLP attention mechanism:
\begin{align}
\mathbf{e}_t &= \tanh(\mathbf{W}_1 \mathbf{h}_t + \mathbf{b}_1) \\
\alpha_t &= \text{softmax}(\mathbf{w}_2^\top \mathbf{e}_t) \\
\mathbf{z}_l &= \sum_{t=1}^T \alpha_t \mathbf{h}_t \in \mathbb{R}^C
\end{align}

This yields a stack $\mathbf{Z} = [\mathbf{z}_1, \ldots, \mathbf{z}_L] \in \mathbb{R}^{L \times C}$, where each $\mathbf{z}_l$ represents the temporally-aggregated representation of layer $l$.

\textbf{Intra-group attention (across neighbouring layers).} To respect the hierarchical depth of the encoder, we split the $L$ layer tokens into contiguous groups of size $g$ (default $g=3$), padding if needed. For each group $\mathbf{G}_k \in \mathbb{R}^{g \times C}$, we apply another attention pooling to obtain a group vector $\mathbf{z}_k'$, followed by a residual refinement block that improves separability while preserving identity:
\begin{align}
\mathbf{z}_k' &= \text{AttnPool}(\mathbf{G}_k) \\
\mathbf{z}_k' &\leftarrow \mathbf{z}_k' + \text{MLP}(\mathbf{z}_k').
\end{align}

This stage lets the model adaptively pick the most informative neighbourhood of layers (e.g., shallow acoustic vs. mid/late phonetic-semantic cues).

\textbf{Inter-group attention (across layer clusters).} We aggregate the set of refined group vectors $\{\mathbf{z}_k'\}$ with a third attention pooling to produce an utterance-level embedding $\mathbf{u} \in \mathbb{R}^C$, then apply a final refinement MLP with residual skip:
\begin{align}
\mathbf{u} &= \text{AttnPool}(\{\mathbf{z}_k'\}) \\
\mathbf{u} &\leftarrow \mathbf{u} + \text{MLP}(\mathbf{u}).
\end{align}

This three-stage pipeline explicitly models temporal $\rightarrow$ intra-group $\rightarrow$ inter-group dependencies, mitigating feature collapse from flat fusion and yielding interpretable attention over time, layers, and layer-groups.

\subsection{Classifier and Projection Head}
The classifier is a regularized MLP (dropout + layer norm) mapping $\mathbf{u}$ to logits $\mathbf{y} \in \mathbb{R}^2$ (self.classifier, LogSoftmax). In parallel, a projection head (self.projection\_head) maps $\mathbf{u}$ to a 256-D space for contrastive learning.

\subsection{Loss Functions}
We train with a classification loss and two regularizers:

Cross-entropy (CE) on logits $\mathbf{y}$ (compatible default).

Margin based Contrastive Loss. Given projected features $\mathbf{f}_i$ and labels, we use temperature-scaled cosine similarity with a margin $m$ to enlarge the gap between positives and negatives while avoiding over-tight clustering:
\begin{equation}
\mathcal{L}_{\text{con}} = \frac{1}{N} \sum_i \max(0, m + \bar{s}_i^{(-)} - \bar{s}_i^{(+)}),
\end{equation}
where $\bar{s}_i^{(+)}$ and $\bar{s}_i^{(-)}$ are mean positive/negative similarities for sample $i$.

The compatible objective (default in code) is:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CL}} + \lambda_{\text{con}} \mathcal{L}_{\text{con}}
\end{equation}

An alternative adaptive objective (provided) swaps CE for Focal and dynamically adjusts $\lambda_{\text{con}}$ by monitoring recent loss trends.

\subsection{Interpretability}
The hierarchical attention mechanism provides interpretability by revealing which temporal frames, layers, and groups contribute most to detection. Attention weights ($\alpha_t$, $\beta_k$, $\gamma$) can be visualized as heatmaps to identify discriminative temporal segments and layer hierarchies.

\section{Experiments}

\subsection{Experimental Setup}
\textbf{Datasets.} We train on ASVspoof 2019 LA (25,380 utterances: 2,580 genuine, 22,800 spoofed) and evaluate on three challenging benchmarks: ASVspoof 2021 LA (148,176 utterances), ASVspoof 2021 DF (533,928 utterances with 100+ unseen generation algorithms), and In-the-Wild (31,779 real-world utterances). We use Equal Error Rate (EER) as the evaluation metric.

\textbf{Implementation.} We employ pre-trained XLS-R 300M as the SSL encoder, extracting features from all 24 transformer layers (dimension 1024). Audio is segmented to 4 seconds (64,600 samples). Our hierarchical attention uses group size 3, attention dimension 128, and residual refinement with hidden dimension 512. Contrastive loss weights are $\lambda_{\text{cont}} = 0.1$ with temperature $\tau = 0.1$. We jointly fine-tune XLS-R and our modules using Adam optimizer (lr=$1 \times 10^{-6}$, weight decay=0.0001, batch size=16) for up to 50 epochs with early stopping (patience=3). RawBoost algorithm 3 is applied for data augmentation. Training is conducted on a single A100 GPU.

\subsection{Main Results}
Table~\ref{tab:main_results} compares our method with state-of-the-art approaches. Our hierarchical attention framework with contrastive learning achieves the best performance across all three datasets, establishing new state-of-the-art results on both DF (1.93\% EER) and In-the-Wild (6.87\% EER). These results demonstrate that explicit modeling of inter-layer dependencies through hierarchical attention effectively exploits complementary information across SSL representation levels.

\begin{table}[h]
\centering
\caption{Comparative Pooled EER (\%) results of our proposed model with others in the ASVspoof2021 DF, LA and In the wild evaluation set. Results are the average obtained from the three runs of each experiment with different random seeds.}
\label{tab:main_results}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{21 LA} & \textbf{21 DF} & \textbf{ITW} \\
\midrule
XLS-53 + LGF & 4.75 & 6.53 & 11.19 \\
XLS-R + Rawnet + AASIST & 4.11 & 2.85 & 10.46 \\
XLS-R + SLS & 3.88 & 2.09 & 8.87 \\
\textbf{XLS-R + Hiercon (Proposed)} & \textbf{2.46} & \textbf{1.93} & \textbf{6.87} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:main_results} shows our hierarchical attention framework establishes new state-of-the-art results across all evaluation benchmarks. On ASVspoof 2021 DF, we achieve 1.93\% and demonstrate substantial improvements on generalization-focused datasets: 36.6\% relative gain over SLS on LA (2.46\% vs 3.88\%) and 22.5\% relative gain on In-the-Wild (6.87\% vs 8.87\%). These results validate that hierarchical modeling of inter-layer dependencies yields more robust, domain-invariant representations than independent layer weighting.

\subsection{Ablation Studies}

\begin{table}[h]
\centering
\caption{Ablation study of the components in our proposed method with an extension to other SSL models.}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{21 LA} & \textbf{21 DF} & \textbf{ITW} \\
\midrule
XLS-R + SLS & 3.88 & 2.09 & 8.87 \\
XLS-R + Hierarchical Attention & 2.97 & 2.13 & 8.81 \\
XLS-R + Hierarchical Attention + Contrastive Learning & \textbf{2.46} & \textbf{1.93} & \textbf{6.87} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:ablation} presents ablation results analyzing the contribution of hierarchical attention and contrastive learning. Starting from the SLS baseline (independent layer weighting), adding hierarchical attention alone provides substantial gains on LA (23.5\% relative improvement) and In-the-Wild (0.7\% improvement), while showing slight degradation on DF. This suggests that hierarchical modeling is particularly effective for cross-domain generalization but requires additional regularization for optimal in-domain performance. The combination of hierarchical attention with margin-based contrastive learning yields the best results across all datasets, with the most dramatic improvement on In-the-Wild (27.7\% relative gain over SLS baseline). The margin-based contrastive loss encourages separation between real and fake embeddings across different domains, addressing the tendency of hierarchical attention to overfit domain-specific patterns. These complementary effects validate our design: hierarchical attention captures multi-level dependencies while contrastive regularization ensures domain-invariant representations.

\subsection{Attention Analysis and Interpretability}
To understand how hierarchical attention contributes to detection, we visualize the learned attention patterns. Figure~\ref{fig:temporal_attention} shows the temporal attention heatmap averaged over 200 spoof samples from the DF evaluation set, revealing which time frames and layers the model focuses on during detection.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{temporal_attention.png}
\caption{Heatmap of attention weight of the temporal attention component by time frame by layer}
\label{fig:temporal_attention}
\end{figure}

Figure~\ref{fig:temporal_attention} exhibits a distinctive pattern where attention concentrates on the middle temporal region (approximately 40-70\% of the audio duration) across most layers. This observation suggests that the model learns to focus on stable, content-rich segments while de-emphasizing potentially noisy boundaries. The beginning and end of audio clips often contain artifacts from segmentation (clips are truncated to 4 seconds), microphone start-up effects, or trailing silence, making the middle portion more reliable for discrimination. Importantly, this attention pattern emerges naturally during training without explicit supervision, demonstrating that the hierarchical attention mechanism successfully identifies the most informative temporal regions for deepfake detection.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{intra_group_attention.png}
\caption{Heatmap of attention weight of the intra group attention component by layer}
\label{fig:intra_attention}
\end{figure}

The intra-group attention analysis (Figure~\ref{fig:intra_attention}) shows distinct adaptive patterns across layer groups. Early groups (Group 1: layers 0-2) assign highest weight to the first layer within the group (L0: 0.438), capturing low-level acoustic features at the entry point where raw signal transformations are most aggressive. Conversely, late groups (Group 8: layers 21-23) strongly favor the deepest layer (L2: 0.572), indicating that high-level abstractions near the classification head provide the most discriminative information. This adaptive layer selection validates our hierarchical design: rather than treating all layers uniformly or selecting only high/low extremes, the model dynamically weights complementary representations at different abstraction levels.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{inter_group_attention.png}
\caption{Heatmap of attention weight of the inter group attention component by layer}
\label{fig:inter_attention}
\end{figure}

The inter-group attention (Figure~\ref{fig:inter_attention}) reveals that mid-level representations dominate detection decisions. Group 5 (layers 12-14) receives the highest weight (0.243), followed by Groups 4 and 6 (0.188 and 0.189). This pattern—with substantial contributions from middle layers and lower weights for shallow (Groups 1-2) and deep (Groups 7-8) extremes—demonstrates that intermediate representations balancing acoustic detail and contextual abstraction are most effective for deepfake detection. Notably, while Group 8 (final layers) shows important within-group structure (Figure~\ref{fig:intra_attention}, L2: 0.572), its inter-group weight (0.030) is relatively low, suggesting the model leverages complementary information across the full layer hierarchy rather than relying solely on final representations. This multi-level attention analysis validates our core hypothesis: explicit hierarchical modeling outperforms both flat fusion and selective high-low combination approaches by systematically exploiting complementary information across all representation levels. The learned patterns—temporal focus on stable regions, adaptive intra-group layer selection, and inter-group emphasis on intermediate features—emerge naturally from end-to-end training without manual feature engineering, providing both performance gains and interpretable insights into the detection mechanism.

\section{Conclusion}
We presented a hierarchical multi-layer attention framework with contrastive learning for robust audio deepfake detection. By explicitly modeling inter-layer dependencies through three-stage attention and incorporating margin-based contrastive regularization, our approach achieves state-of-the-art performance on ASVspoof 2021 DF (1.93\% EER) and In-the-Wild (6.87\% EER) datasets. Ablation studies confirm that hierarchical attention and contrastive learning provide substantial complementary benefits, while attention visualization reveals interpretable detection patterns. Future work will explore extension to other SSL models and real-time deployment scenarios.

%%
%% Bibliography - ACM Reference Format
%%
\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{16}

\bibitem{babu2021xlsr}
A.~Babu, C.~Wang, A.~Tjandra, K.~Lakhotia, Q.~Xu, N.~Goyal, K.~Singh, P.~von Platen, Y.~Saraf, J.~Pino, A.~Baevski, A.~Conneau, and M.~Auli. 2021.
XLS-R: Self-supervised cross-lingual speech representation learning at scale.
arXiv preprint arXiv:2111.09296.

\bibitem{muller2022does}
N.~M. Müller, P.~Czempin, F.~Dieckmann, A.~Froghyar, and K.~Böttinger. 2022.
Does audio deepfake detection generalize?
arXiv preprint arXiv:2203.16263.

\bibitem{combei2025unmasking}
D.~Combei, A.~Stan, D.~Oneata, N.~Müller, and H.~Cucu. 2025.
Unmasking real-world audio deepfakes: A data-centric approach.
arXiv preprint arXiv:2506.09606.

\bibitem{babu2022xlsr}
A.~Babu, C.~Wang, A.~Tjandra, K.~Lakhotia, Q.~Xu, N.~Goyal, K.~Singh, P.~von Platen, Y.~Saraf, J.~Pino, A.~Baevski, A.~Conneau, and M.~Auli. 2022.
XLS-R: Self-supervised cross-lingual speech representation learning at scale.
In \emph{Interspeech 2022: 23rd Annual Conference of the International Speech Communication Association}. H.~Ko and J.~H.~L. Hansen (Eds.). ISCA, 2278--2282.

\bibitem{chen2022wavlm}
S.~Chen, C.~Wang, Z.~Chen, Y.~Wu, S.~Liu, Z.~Chen, J.~Li, N.~Kanda, T.~Yoshioka, X.~Xiao, J.~Wu, L.~Zhou, S.~Ren, Y.~Qian, Y.~Qian, J.~Wu, M.~Zeng, X.~Yu, and F.~Wei. 2022.
WavLM: Large-scale self-supervised pre-training for full stack speech processing.
\emph{IEEE Journal of Selected Topics in Signal Processing} 16, 6 (2022), 1505--1518.
\url{https://doi.org/10.1109/JSTSP.2022.3188113}

\bibitem{hsu2021hubert}
W.-N. Hsu, B.~Bolte, Y.-H.~H. Tsai, K.~Lakhotia, R.~Salakhutdinov, and A.~Mohamed. 2021.
HuBERT: Self-supervised speech representation learning by masked prediction of hidden units.
\emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing} 29 (2021), 3451--3460.
\url{https://doi.org/10.1109/TASLP.2021.3122291}

\bibitem{cox2023voice}
J.~Cox. 2023.
How I broke into a bank account with an AI-generated voice.
\emph{Vice} (July 30, 2023).
\url{https://www.vice.com/en/article/dy7axa/how-i-broke-into-a-bank-account-with-an-ai-generated-voice}

\bibitem{nautsch2021asvspoof}
A.~Nautsch, X.~Wang, N.~Evans, T.~H. Kinnunen, V.~Vestman, M.~Todisco, H.~Delgado, M.~Sahidullah, J.~Yamagishi, and K.~A. Lee. 2021.
ASVspoof 2019: Spoofing countermeasures for the detection of synthesized, converted and replayed speech.
\emph{IEEE Transactions on Biometrics, Behavior, and Identity Science} 3, 2 (2021), 252--265.
\url{https://doi.org/10.1109/TBIOM.2021.3059479}

\bibitem{todisco2018integrated}
M.~Todisco, H.~Delgado, K.~A. Lee, M.~Sahidullah, N.~Evans, T.~Kinnunen, and J.~Yamagishi. 2018.
Integrated presentation attack detection and automatic speaker verification: Common features and Gaussian back-end fusion.
In \emph{Interspeech 2018: 19th Annual Conference of the International Speech Communication Association}. ISCA, 1103--1107.

\bibitem{chen2010speaker}
L.-W. Chen, W.~Guo, and L.-R. Dai. 2010.
Speaker verification against synthetic speech.
In \emph{2010 7th International Symposium on Chinese Spoken Language Processing}. IEEE, 309--312.
\url{https://doi.org/10.1109/ISCSLP.2010.5684898}

\bibitem{ravanelli2018speaker}
M.~Ravanelli and Y.~Bengio. 2018.
Speaker recognition from raw waveform with SincNet.
In \emph{2018 IEEE Spoken Language Technology Workshop (SLT)}. IEEE, 1021--1028.
\url{https://doi.org/10.1109/SLT.2018.8639585}

\bibitem{fu2022fastaudio}
Q.~Fu, Z.~Teng, J.~White, M.~E. Powell, and D.~C. Schmidt. 2022.
FastAudio: A learnable audio front-end for spoof speech detection.
In \emph{ICASSP 2022: 2022 IEEE International Conference on Acoustics, Speech and Signal Processing}. IEEE, 3693--3697.
\url{https://doi.org/10.1109/ICASSP43922.2022.9746083}

\bibitem{xiao2025layer}
Y.~Xiao and N.~T. Vu. 2025.
Layer-wise decision fusion for fake audio detection using XLS-R.
In \emph{Interspeech 2025}. ISCA, 5618--5622.

\bibitem{todisco2018integrated2}
M.~Todisco, H.~Delgado, K.~A. Lee, M.~Sahidullah, N.~Evans, T.~Kinnunen, and J.~Yamagishi. 2018.
Integrated presentation attack detection and automatic speaker verification: Common features and Gaussian back-end fusion.
In \emph{Interspeech 2018: 19th Annual Conference of the International Speech Communication Association}. ISCA, 1103--1107.

\bibitem{jung2022aasist}
J.~Jung, H.-S. Heo, H.~Tak, H.~Shim, J.~S. Chung, B.-J. Lee, H.-J. Yu, and N.~Evans. 2022.
AASIST: Audio anti-spoofing using integrated spectro-temporal graph attention networks.
In \emph{ICASSP 2022: 2022 IEEE International Conference on Acoustics, Speech and Signal Processing}. IEEE, 6367--6371.
\url{https://doi.org/10.1109/ICASSP43922.2022.9746401}

\bibitem{zhang2024audio}
Q.~Zhang, S.~Wen, and T.~Hu. 2024.
Audio deepfake detection with self-supervised XLS-R and SLS classifier.
In \emph{Proceedings of the 32nd ACM International Conference on Multimedia}. ACM, 6765--6773.
\url{https://doi.org/10.1145/3664647.3681029}

\end{thebibliography}

\end{document}