%% For WWW 2026 Short Papers
%% Use sigconf, review for submission
\documentclass[sigconf, review]{acmart}

%% Add subcaption package for subfigures
\usepackage{subcaption}
\usepackage{adjustbox}
\newcommand{\mike}[1]{\textcolor{blue}{[Mike: #1]}}

%% Rights management information. 
%% For submission, you can leave this commented out
% \setcopyright{acmlicensed}
% \copyrightyear{2026}
% \acmYear{2026}
% \acmConference[WWW '26]{Proceedings of the ACM Web Conference 2026}{April 28--May 2, 2026}{Sydney, Australia}
% \acmBooktitle{Proceedings of the ACM Web Conference 2026 (WWW '26), April 28--May 2, 2026, Sydney, Australia}
% \acmDOI{10.1145/nnnnnnn.nnnnnnn}
% \acmISBN{978-1-4503-XXXX-X/18/06}

\begin{document}

\title{HierCon: Hierarchical Contrastive Attention \\for Audio Deepfake Detection}

%%
%% Authors - replace with your actual information
%%
\author{Nicholas Liang}
\email{nnliang@unimelb.edu.au}
\affiliation{%
  \institution{University of Melbourne}
  \city{Melbourne}
  \country{Australia}
}

\author{Soyeon Caren Han}
\email{caren.han@unimelb.edu.au}
\affiliation{%
  \institution{University of Melbourne}
  \city{Melbourne}
  \country{Australia}
}

\author{Mike Wang}
\email{mike.wang@unimelb.edu.au}
\affiliation{%
  \institution{University of Melbourne}
  \city{Melbourne}
  \country{Australia}
}

\author{Christopher Leckie}
\email{caleckie@unimelb.edu.au}
\affiliation{%
  \institution{University of Melbourne}
  \city{Melbourne}
  \country{Australia}
}

\renewcommand{\shortauthors}{Liang, et al.}


%%
%% Abstract
%%
\begin{abstract}
Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93\% and 6.87\% EER), improving over independent layer weighting by 36.6\% and 22.5\% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.
% Generative AI advances have made synthetic speech nearly indistinguishable from genuine voices, posing critical security challenges. While self-supervised models like XLS-R provide rich representations across transformer layers, current methods treat layers independently, failing to capture inter-layer dependencies. We propose a hierarchical layer attention framework with contrastive learning that explicitly models dependencies through three stages: temporal attention within layers, intra-group attention among neighboring layers, and inter-group attention across layer clusters. To enhance cross-domain generalization, we incorporate margin-based contrastive learning that encourages domain-invariant embeddings. Evaluated on ASVspoof 2021 DF (1.93\% EER) and In-the-Wild (6.87\% EER), our approach achieves state-of-the-art performance with 36.6\% and 22.5\% relative improvements over independent layer weighting. Attention visualization reveals interpretable patterns validating our design, with the model learning to focus on stable temporal regions and adaptively weighting complementary layer representations.
\end{abstract}

%%
%% CCS Concepts and Keywords
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002978.10002991.10002992</concept_id>
       <concept_desc>Security and privacy~Social aspects of security and privacy</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Speech recognition}
\keywords{audio deepfake detection, anti-spoofing, self-supervised learning}

\settopmatter{authorsperrow=4}
\maketitle

\section{Introduction}
Recent progress in Text-to-Speech (TTS) and Voice Conversion (VC) systems has made synthetic speech increasingly realistic, enabling attackers to generate speech that closely mimics genuine human voices~\cite{muller2022does}. Such advances raise significant security and trust concerns in voice authentication, digital forensics, and online communication~\cite{jung2022aasist}. Early deepfake detection relied on handcrafted spectral or cepstral features~\cite{todisco2018integrated}, but the field has rapidly shifted toward deep learning architectures~\cite{ravanelli2018speaker,jung2022aasist} and self-supervised learning (SSL) models~\cite{chen2022wavlm,babu2022xlsr}, which offer richer multi-layer speech representations. Among these, XLS-R-based approaches have demonstrated strong results; for example, Sensitive Layer Selection (SLS) achieves 2.09\% EER on ASVspoof 2021 DF by learning scalar weights across transformer layers~\cite{zhang2024audio}.
However, methods such as SLS treat each transformer layer independently, overlooking two key sources of discriminative information. First, temporal dynamics within layers are ignored so every frame receives equal weight despite the fact that only certain regions carry synthesis artefacts. Second, inter-layer dependencies are not modelled—shallow layers encode acoustic cues while deeper layers capture semantic or prosodic signals, and their interactions are often essential for detecting modern deepfakes. Ignoring these dependencies can lead to feature homogenisation and poorer generalisation to cross-domain generation pipelines~\cite{xiao2025layer}.

To address these limitations, we introduce a hierarchical layer attention framework with margin-based contrastive learning. Our approach explicitly models dependencies at three levels:
(1) temporal attention, which identifies informative frame regions within each layer;
(2) intra-group attention, which captures complementary information across neighbouring layers; and
(3) inter-group attention, which integrates evidence across broader layer clusters.
To further improve robustness, we incorporate contrastive regularisation that encourages domain-invariant embedding geometry, mitigating overfitting to dataset-specific artefacts and enabling better generalisation to diverse recording conditions and deepfake generation methods.
Evaluated on ASVspoof 2021 DF and In-the-Wild benchmarks, our method achieves state-of-the-art performance, reducing EER to 1.93\% and 6.87\% respectively, representing 36.6\% and 22.5\% relative improvements over independent layer weighting.

Our contributions are summarised as follows:

\noindent\textbf{1) Hierarchical Layer Attention Framework}: We propose a three-stage attention architecture that models temporal, intra-layer, and inter-layer dependencies, allowing the model to exploit complementary cues across the full representational hierarchy rather than treating layers independently.
\textbf{2) Contrastive Regularisation for Domain-Invariant Learning}: We introduce margin-based contrastive learning during fine-tuning to impose explicit geometric constraints on layer representations. Unlike prior SSL-based detectors~\cite{zhang2024audio,xiao2025layer}, our approach jointly optimises classification and contrastive objectives, improving cross-domain robustness and preventing reliance on dataset-specific artefacts.
\textbf{3) Interpretability and Robust Generalisation}: Our hierarchical attention structure naturally provides interpretable insights into which temporal regions and layer groups drive predictions. Experiments on ASVspoof 2021 DF and In-the-Wild demonstrate strong generalisation and outperform prior layer-fusion approaches.



% Recent advances in Text-to-Speech (TTS) and Voice Conversion (VC) have enabled synthesis of speech nearly indistinguishable from genuine voices~\cite{muller2022does}, raising urgent security concerns in voice-based authentication and forensic applications~\cite{combei2025unmasking,cox2023voice}. While early detection methods relied on handcrafted features~\cite{todisco2018integrated}, recent approaches leverage deep learning~\cite{ravanelli2018speaker,jung2022aasist} and self-supervised learning (SSL) models~\cite{chen2022wavlm,babu2022xlsr}. Among these, XLS-R-based methods have shown strong performance, with the SLS (Sensitive Layer Selection) module achieving 2.09\% EER on ASVspoof 2021 DF by applying learned scalar weights to each transformer layer independently~\cite{zhang2024audio}.

% However, SLS treats layers in isolation, ignoring two critical aspects: (1) \textit{temporal dynamics}---all frames within a layer receive equal weight regardless of informativeness, and (2) \textit{inter-layer dependencies}---complementary relationships across abstraction levels (e.g., shallow acoustic vs. deep semantic features) cannot be captured. This independence assumption leads to feature homogenization and domain overfitting~\cite{xiao2025layer}.

% We propose a hierarchical layer attention framework with contrastive learning that explicitly models dependencies across temporal frames, neighboring layers, and layer clusters. Our three-stage attention mechanism---temporal, intra-group, and inter-group---dynamically identifies and integrates complementary information across the full representational hierarchy. Combined with margin-based contrastive regularization for domain-invariant learning, our approach achieves state-of-the-art performance on ASVspoof 2021 DF (1.93\% EER) and In-the-Wild (6.87\% EER), with 36.6\% and 22.5\% relative improvements over independent layer weighting.

% Our main contributions are summarised as follows:

% \textbf{1) Hierarchical Layer Attention Framework.}
% We introduce a novel hierarchical attention architecture that explicitly models dependencies across temporal frames, neighbouring transformer layers, and layer groups within self-supervised speech encoders. This design enables the model to leverage complementary cues across representation levels rather than treating layers independently.

% \textbf{2) Contrastive Regularisation for Domain-Invariant Learning.}
% To enhance robustness across cross-domain generation methods and recording conditions, we integrate a margin-based contrastive objective with supervised classification. Unlike existing SSL-based deepfake detectors that rely solely on cross-entropy loss~\cite{zhang2024audio,xiao2025layer}---which optimizes for correct label prediction but does not explicitly enforce embedding geometry---our dual-objective approach introduces a critical novelty: \textit{explicit geometric constraints on the learned representation space}. While prior work applies contrastive learning during SSL pre-training (e.g., Wav2Vec 2.0's masked prediction), we are the first to apply margin-based contrastive regularization during task-specific fine-tuning of hierarchical layer representations. This design explicitly encourages domain-invariant embeddings where real and fake speech remain separable across diverse acoustic conditions, directly addressing the domain overfitting challenge where models memorize training-specific artifacts (e.g., particular vocoder signatures) rather than learning generalizable spoofing cues.

% \textbf{3) Interpretability and Robust Generalisation.}
% The attention mechanism provides intrinsic interpretability by revealing which temporal regions and layer hierarchies contribute most to classification decisions. Extensive experiments on ASVspoof 2021 DF and In-the-Wild datasets demonstrate competitive performance and superior cross-domain generalisation compared with prior layer-fusion approaches.

\section{Proposed Method}
\subsection{Preliminaries}
\textbf{SSL-based Audio Deepfake Detection.}
A popular line of detection methods leverages pretrained SSL audio models and has shown strong performance. Among these, XLS-R is the most common choice. It is a large-scale cross-lingual representation model built on Wav2Vec 2.0 and trained on 128 languages to capture universal acoustic and linguistic features. To encode a raw waveform, it first extracts frame level representations at predefined intervals, which are then processed by a $L$-layer transformer network to obtain contextualised hidden states $\mathbf{H} = [\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_L]$.

\noindent\textbf{Leveraging Multi-layer Representations.}
Existing SSL-based detectors typically utilise all hidden states to capture a more comprehensive range of audio features across multiple levels, including shallow acoustic, mid-level prosodic and deep semantic features. However, a key limitation of these methods is they assume all hidden states carry equally important information that can be summarised through uniform weighting. This can dilute discriminative signals, and ignoring inter-layer dependencies limits the ability to capture the complementary relationships across and within each level. This motivates HierCon, which dynamically models these dependencies across multiple granularities.


\subsection{HierCon}
% Existing SSL-based detectors apply independent scalar weights to transformer layers~\cite{zhang2024audio}, but uniform frame weighting dilutes discriminative signals and ignoring inter-layer dependencies prevents capturing complementary relationships across shallow acoustic, mid-level prosodic, and deep semantic features.
% To fully capture discriminative information from the SLR-S features, we introduce \textbf{Hier}archical \textbf{Con}trastive Attention (HierCon) for audio deepfake detection. At its core is a hierarchical attention module tailored to the SLR-S multi-layer representations. The layers are grouped to enable hierarchical learning, where \textit{intra-group attention} first captures relationships within each group of neighbouring layers (8 groups of 3), and \textit{inter-group attention} then models dependencies across groups to integrate complementary evidence. To better guide the hierarchical attention toward learning discriminative features, a margin-based contrastive loss that strengthens intra class cohesion and inter class separation in the representation space is introduced to regularise the entropy-based training.
To fully capture discriminative information from XLS-R features, we introduce \textbf{Hier}archical \textbf{Con}trastive Attention (HierCon). At its core is a hierarchical attention module that groups XLS-R's 24 layers into 8 groups of 3 consecutive layers. This grouping scheme balances two competing objectives: groups must be small enough (3 layers) to capture homogeneous abstraction levels~\cite{xiao2025layer} where neighbouring layers exhibit similar feature statistics, yet large enough to provide sufficient intra-group diversity for attention to be meaningful. With 8 groups spanning 24 layers, we obtain coarse-grained hierarchical clusters that correspond to early acoustic (groups 1--3), mid-level prosodic (groups 4--6), and high-level semantic representations (groups 7--8). This enables \textit{intra-group attention} to model local dependencies and \textit{inter-group attention} to integrate complementary evidence across the full abstraction hierarchy. Combined with margin-based contrastive learning, this design learns generalizable spoofing patterns rather than dataset-specific artifacts.

% We propose a hierarchical attention framework operating through three stages: \textit{temporal attention} weights discriminative frames within layers, \textit{intra-group attention} models relationships among neighboring layers (8 groups of 3), and \textit{inter-group attention} integrates evidence across layer clusters. Combined with margin-based contrastive learning (Section 2.5), this design learns generalizable spoofing patterns rather than dataset-specific artifacts.



\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{architecture_diagram.drawio.png}
    \caption{Overall architecture of our hierarchical layer attention framework.}
    \label{fig:architecture}
\end{figure}

% The XLS-R 300M SSL front-end extracts 24-layer transformer representations, which are processed through three-stage hierarchical attention: temporal attention weights discriminative frames within each layer, intra-group attention (intra-cluster) models dependencies among neighboring layers at similar abstraction levels, and inter-group attention (inter-cluster) integrates evidence across layer groups. Dual training objectives---classification and contrastive learning---ensure both accurate predictions and domain-invariant embeddings.

% \subsection{SSL Feature Extraction}
% The XLS-R model is a large-scale cross-lingual self-supervised speech representation framework built upon Wav2Vec 2.0. It is trained across 128 languages to capture universal acoustic and linguistic features. Given a raw speech waveform $\mathbf{x}$, a convolutional feature encoder first extracts latent representations $\mathbf{z} \in \mathbb{R}^{T \times 1024}$ at 20ms intervals using a receptive field of 25ms. These frame-level embeddings are then passed through a 24-layer transformer network to obtain contextualized hidden states $\mathbf{h}_l$, where each layer $l$ captures progressively higher-level abstractions of the speech signal. The complete stack of representations is denoted as
% \begin{equation}
% \mathbf{H} = [\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_L], \quad L = 24.
% \end{equation}

% XLS-R is a large-scale cross-lingual self-supervised speech model built upon Wav2Vec 2.0, trained across 128 languages. Given raw speech waveform $\mathbf{x}$, it extracts 24-layer transformer representations $\mathbf{H} = [\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_{24}]$ where $\mathbf{h}_l \in \mathbb{R}^{T \times 1024}$ denotes contextualized embeddings at layer $l$. We fine-tune XLS-R jointly with task-specific modules to obtain layer-wise representations for hierarchical attention and contrastive learning.

\noindent\textbf{1) Hierarchical Attention.}
% Our hierarchical attention mechanism operates in three stages, progressively aggregating information from frame-level to utterance-level representations while explicitly modeling inter-layer dependencies.
HierCon is designed to capture dependencies hierarchically through three stages by applying temporal attention within each layer, intra-group attention among neighbouring layers, and inter-group attention across layer groups. 

% In \textit{Stage 1. Temporal Attention}, in order to emphasize informative frames capturing discriminative artifacts while down-weighting benign content, each layer $l$ applies learnable attention:
In \textit{Stage 1. Temporal Attention}, in order to emphasize informative frames capturing discriminative artifacts while down-weighting benign content, each layer $l$ applies learnable attention over $T$ temporal frames. Given frame-level hidden states $\mathbf{h}_t \in \mathbb{R}^{1024}$ for $t = 1, \ldots, T$, we compute attention weights via a two-layer MLP with learnable parameters $\mathbf{W}_1 \in \mathbb{R}^{128 \times 1024}$, $\mathbf{b}_1 \in \mathbb{R}^{128}$, and $\mathbf{w}_2 \in \mathbb{R}^{128}$:

\begin{equation}
\mathbf{e}_t = \tanh(\mathbf{W}_1 \mathbf{h}_t + \mathbf{b}_1), \quad
\alpha_t = \text{softmax}(\mathbf{w}_2^\top \mathbf{e}_t), \quad
\mathbf{z}_l = \sum_{t=1}^T \alpha_t \mathbf{h}_t
\end{equation}
yielding layer tokens $\mathbf{Z} = [\mathbf{z}_1, \ldots, \mathbf{z}_{24}] \in \mathbb{R}^{24 \times 1024}$.
% \mike{Nic, please make sure your define all parameters. }

In \textit{Stage 2. Intra-Group Attention}, in order to exploit complementary information within similar abstraction levels, we partition 24 layers into eight groups of three ($g=3$):
$\mathbf{z}_k' = \text{AttnPool}(\mathbf{G}_k) + \text{MLP}(\text{AttnPool}(\mathbf{G}_k))$,
where $\mathbf{G}_k \in \mathbb{R}^{3 \times 1024}$ contains neighboring layer tokens for group $k \in \{1, \ldots, 8\}$. Early groups emphasize acoustic artifacts while late groups capture semantic cues.
% where $\mathbf{G}_k \in \mathbb{R}^{3 \times 1024}$ contains neighboring layer tokens. Early groups emphasize acoustic artifacts while late groups capture semantic cues.

In \textit{Stage 3. Inter-Group Attention}, different generation techniques manifest artifacts at different abstraction levels. Aggregating refined group vectors $\{\mathbf{z}_k'\}_{k=1}^8$ yields utterance embedding:
\begin{align}
\mathbf{u} = \text{AttnPool}(\{\mathbf{z}_k'\}) + \text{MLP}(\text{AttnPool}(\{\mathbf{z}_k'\}))
\end{align}
Adaptive weights identify which abstraction levels provide discriminative evidence for each sample.

\noindent\textbf{2) Classifier and Projection Head}
% The classifier is a regularized MLP (dropout + layer norm) mapping $\mathbf{u}$ to logits $\mathbf{y} \in \mathbb{R}^2$ . In parallel, a projection head maps $\mathbf{u}$ to a 256-D space for contrastive learning.
The classifier is a regularized MLP (dropout + layer norm) mapping $\mathbf{u} \in \mathbb{R}^{1024}$ to logits $\mathbf{y} \in \mathbb{R}^2$. In parallel, a projection head---implemented as a two-layer MLP with hidden dimension 512 and ReLU activation---maps $\mathbf{u}$ to a 256-dimensional space for contrastive learning, yielding embeddings $\mathbf{f}_i \in \mathbb{R}^{256}$. This 4$\times$ dimensionality reduction (1024$\to$256) is crucial: it creates a geometric bottleneck that forces the model to learn compact, discriminative representations essential for metric learning, while preventing gradient interference between the classification and contrastive objectives which operate on different embedding spaces. The projection head is discarded at inference, with only the classifier pathway used for prediction.

\noindent\textbf{3) Loss Functions}
We jointly use a binary cross-entropy (BCE) loss and a margin-based contrastive loss to stabilize training and shape the embedding geometry. The BCE term drives accurate real-vs-fake classification, while the contrastive term mitigates feature collapse from entropy-based training and reduces overfitting to dataset-specific artifacts.
Given a batch of $N$ samples with utterance embeddings $\{\mathbf{f}_i\}_{i=1}^N$, we first project them into a 256-dimensional space and compute cosine similarities. For each anchor $i$, we define the positive set $\mathcal{P}_i$ as samples from the same class (real or fake) and the negative set $\mathcal{N}_i$ as samples from the opposite class. We then compute the mean similarities
\begin{equation}
\bar{s}_i^{(+)} = \frac{1}{|\mathcal{P}_i|} \sum_{j \in \mathcal{P}_i} \cos(\mathbf{f}_i, \mathbf{f}_j), \quad
\bar{s}_i^{(-)} = \frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} \cos(\mathbf{f}_i, \mathbf{f}_j),
\end{equation}
and enforce a margin $m$ between positive and negative similarities:
\begin{equation}
\mathcal{L}_{\text{con}} = \frac{1}{N} \sum_{i=1}^N \max\bigl(0,\, m + \bar{s}_i^{(-)} - \bar{s}_i^{(+)}\bigr).
\end{equation}

\noindent The overall training objective is
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_{\text{con}} \mathcal{L}_{\text{con}},
\end{equation}
where $\mathcal{L}_{\text{CE}}$ denotes the BCE loss and $\lambda_{\text{con}}$ controls the strength of contrastive regularisation. We set $\lambda_{\text{con}} = 0.1$ based on systematic validation experiments over $\{0.01, 0.05, 0.1, 0.5, 1.0\}$: larger values (0.5–1.0) overemphasise the contrastive term and degrade classification accuracy, while smaller values ($<0.05$) yield only marginal improvements over baseline. Critically, we apply constant weighting throughout training rather than scheduling (e.g., warm-up or annealing). This design choice ensures both objectives jointly shape the representation space from initialization, preventing the model from first converging to a classification-optimized geometry that is difficult to reshape later. Scheduled weighting risks suboptimal local minima where either objective dominates at different training phases, reducing the synergy between discriminative classification and domain-invariant embedding structure.

% To avoid feature collapse caused by entropy-based training, a margin-based contrastive loss is adopted to regularise the binary cross-entropy (BCE) loss and maintain the intended geometric structure of the feature space. Given a batch of $N$ samples with embeddings $\{\mathbf{f}_i\}_{i=1}^N$, we compute cosine similarities and enforce margin $m$ between positive and negative pairs:
% \begin{equation}
% \mathcal{L}_{\text{con}} = \frac{1}{N} \sum_{i=1}^N \max(0, m + \bar{s}_i^{(-)} - \bar{s}_i^{(+)})
% \end{equation}
% where $\bar{s}_i^{(+)} = \frac{1}{|\mathcal{P}_i|} \sum_{j \in \mathcal{P}_i} \cos(\mathbf{f}_i, \mathbf{f}_j)$ denotes the mean cosine similarity to positive pairs $\mathcal{P}_i$, and $\bar{s}_i^{(-)} = \frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} \cos(\mathbf{f}_i, \mathbf{f}_j)$ denotes the mean similarity to different-class samples (negative pairs $\mathcal{N}_i$). The overall objective is:

% To prevent domain overfitting, we combine classification (cross-entropy $\mathcal{L}_{\text{CE}}$) with margin-based contrastive learning. Utterance embeddings are projected to $\mathbf{f}_i \in \mathbb{R}^{256}$, enforcing margin $m$ between positive and negative pairs:
% \begin{equation}
% \mathcal{L}_{\text{con}} = \frac{1}{N} \sum_i \max(0, m + \bar{s}_i^{(-)} - \bar{s}_i^{(+)})
% \end{equation}
% where $\bar{s}_i^{(+)}$ and $\bar{s}_i^{(-)}$ denote mean similarities to same-class and different-class samples. Total loss $\mathcal{L} = \mathcal{L}_{\text{CE}} + \lambda_{\text{con}} \mathcal{L}_{\text{con}}$ (with $\lambda_{\text{con}}=0.1$). We set $\lambda_{\text{con}}=0.1$ through validation: higher values (0.5-1.0) hurt classification accuracy while lower values ($<$0.05) provide insufficient regularisation. We apply constant weighting (no scheduling) to jointly shape the representation space from the start, preventing convergence to suboptimal geometries.



\noindent\textbf{4) Interpretability}
The hierarchical attention mechanism provides interpretability by revealing which temporal frames, layers, and groups contribute most to detection. Attention weights ($\alpha_t$, $\beta_k$, $\gamma$) can be visualized as heatmaps to identify discriminative temporal segments and layer hierarchies.

\section{Experiments}
We evaluate HierCon across three dimensions: (i) overall detection performance compared to state-of-the-art baselines, (ii) ablation analysis to isolate the contributions of hierarchical attention and contrastive learning, and (iii) interpretability analysis through multi-stage attention visualization.  
% We conduct comprehensive experiments to evaluate our hierarchical attention framework across multiple dimensions: comparison with state-of-the-art approaches, ablation analysis of individual components, and interpretability through attention visualization.

\noindent\textbf{Experimental Setup}
All models are trained using the ASVspoof 2019 LA subset and evaluated on three benchmarks with varying levels of cross-domain difficulty: ASVspoof 2021 LA (148,176 utterances), ASVspoof 2021 DF (533,928 utterances spanning over 100 deepfake generation pipelines), and In-the-Wild (ITW) (31,779 real-world samples). Detection performance is reported using Equal Error Rate (EER\%), the standard metric in audio anti-spoofing research.
We adopt XLS-R 300M as our feature backbone, extracting hidden states from all 24 transformer layers and segmenting audio into 4-second windows. HierCon groups layers into clusters of three (8 groups total) and applies temporal, intra-group, and inter-group attention with dimensions 128 and 512 for attention and feed-forward components, respectively.
Training uses the Adam optimizer with learning rate $1 \times 10^{-6}$, batch size 16, and early stopping over 50 epochs. We apply RawBoost augmentation~\cite{tak2022rawboost} and average all reported results over three randomized seeds. Contrastive learning uses batch-level sampling, where positive pairs share spoofing class (real $\longleftrightarrow$ real or fake $\longleftrightarrow$ fake) and negative pairs differ. Average similarity scores form batch-level positive and negative constraints in the contrastive margin objective.

%  We train on ASVspoof 2019 LA and evaluate on ASVspoof 2021 LA (148,176 utterances), 2021 DF (533,928 utterances with 100+ cross-domain generation algorithms), and In-the-Wild (31,779 real-world utterances). We report Equal Error Rate (EER\%).

% We use pre-trained XLS-R 300M, extracting features from all 24 transformer layers. Audio is segmented to 4 seconds. Our hierarchical attention uses group size 3 (8 groups), attention dimension 128, and hidden dimension 512. We set $\lambda_{\text{cont}} = 0.1$ and $\tau = 0.1$. Training uses Adam optimizer (lr=$1 \times 10^{-6}$, batch size=16) for up to 50 epochs with early stopping. RawBoost algorithm 3~\cite{tak2022rawboost} provides data augmentation. Results are averaged over 3 runs.

% For contrastive learning, positive pairs are samples from the same class (both real or both fake) within a batch, while negative pairs consist of cross-class samples (real vs. fake). We compute similarities across all within-batch pairs, averaging same-class and different-class scores to form $\bar{s}_i^{(+)}$ and $\bar{s}_i^{(-)}$ respectively. This within-batch sampling strategy ensures diverse exposure to different generation methods during training.

\subsection{Overall Performance}
\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.32\textwidth}
    \centering
\includegraphics[width=\textwidth]{Temporal attention.png}
    \caption{Temporal Attention (Stage 1)}
    \label{fig:temporal_attention}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Intra group attention.png}
    \caption{Intra-Group Attention (Stage 2)}
    \label{fig:intra_attention}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{Inter group attention.png}
    \caption{Inter-Group Attention (Stage 3)}
    \label{fig:inter_attention}
\end{subfigure}
\caption{Hierarchical attention patterns averaged over 200 DF samples. (a) Temporal attention concentrates on middle regions (40-70\% duration), avoiding boundary artifacts. (b) Intra-group attention shows adaptive layer selection: early groups favor first layers (L0: 0.438), late groups favor deep layers (L2: 0.572). (c) Inter-group attention reveals mid-level dominance with Group 5 (layers 12-14) receiving highest weight (0.243). }
\label{fig:attention_composite}
\end{figure*}



Table~\ref{tab:main_results} presents pooled EERs across the three datasets with comparisons to recent detector families, including Wav2Vec/WavLM-based systems, one-class detection frameworks, and the self-supervised XLS-R SLS~\cite{zhang2024audio} baseline. 
HierCon establishes new state-of-the-art performance trends under cross-domain evaluation: 

\noindent\textbf{21 DF(Deepfake)}: HierCon achieves 1.93\% EER, surpassing the strong XLS-R + SLS baseline (2.09\%) and outperforming OCKD (2.27\%) and WavLM-based systems. The improvement is particularly impactful given the diversity of DF generation techniques.

\noindent\textbf{21 LA}: With 2.46\% EER, HierCon narrows the gap with highly specialized artifact-aware detectors (such as AASIST), while improving substantially over the XLS-R baseline (3.88\%).

\noindent\textbf{In-the-Wild}: HierCon reduces EER to 6.87\%, a 22.5 \% relative improvement over XLS-R + SLS (8.87\%).

% These results reinforce the hypothesis that modeling layer dependencies hierarchically yields more robust representations than independent, scalar layer weighting. 

% We compare our method against recent SSL-based detectors, including XLS-53 with Layer-wise Gated Fusion (LGF), XLS-R combined with RawNet and AASIST, and the state-of-the-art SLS baseline~\cite{zhang2024audio}. Table~\ref{tab:main_results} presents the results.

\begin{table}[t]
\centering
\caption{Pooled EER (\%) on ASVspoof 2021 benchmarks. Results averaged over 3 runs. Bold indicates best performance. $^\dagger$ indicates results reported from original papers.}
\label{tab:main_results}
\begin{tabular*}{\columnwidth}{@{\extracolsep{\fill}}lccc@{}}
\toprule
\textbf{Model} & \textbf{21 LA} & \textbf{21 DF} & \textbf{ITW} \\
\midrule
Wav2Vec+LogReg~\cite{baevski2020wav2vec}$^\dagger$ & - & - & 7.20 \\
WavLM+ASP~\cite{chen2022wavlm}$^\dagger$  & 3.31 & 4.47 & -\\
Wav2Vec+Linear~\cite{baevski2020wav2vec}$^\dagger$  & 3.63 & 3.65 & 16.17\\
WavLM+AttM~\cite{chen2022wavlm}$^\dagger$ & 3.50 & 3.19 & - \\
Wav2Vec+AASIST~\cite{jung2022aasist}$^\dagger$  & 0.82 & 2.85 & -\\
FTDKD~\cite{sahidullah2015comparison}$^\dagger$ & 2.96 & 2.82 & - \\
Wav2Vec+AASIST2~\cite{jung2022aasist}$^\dagger$  & 1.61 & 2.77 & -\\
WavLM+MFA~\cite{chen2022wavlm}$^\dagger$ & 5.08 & 2.56 & - \\
OCKD~\cite{lu2024one}$^\dagger$  & 0.90 & 2.27 & 7.86\\
OC+ACS~\cite{lavrentyeva2019stc}$^\dagger$  & 1.30 & 2.19 & -\\
XLS-R + SLS~\cite{zhang2024audio} & 3.88 & 2.09 & 8.87 \\
\midrule
% \multicolumn{4}{l}{\textit{Our Method}} \\
% \midrule
\textbf{XLS-R + Hiercon (Ours)} & \textbf{2.46} & \textbf{1.93} & \textbf{6.87} \\
\bottomrule
\end{tabular*}
\end{table}

% Our hierarchical attention framework demonstrates strong performance across ASVspoof 2021 evaluation benchmarks. Table~\ref{tab:main_results} compares our method with recent state-of-the-art approaches that combine self-supervised learning models with various architectural innovations.

% On the challenging DF dataset with 100+ cross-domain generation algorithms, our method achieves 1.93\% EER, outperforming most existing approaches including XLS-R + SLS (2.09\%), OCKD (2.27\%), and various WavLM-based methods. This demonstrates the effectiveness of hierarchical attention in capturing complementary artifacts across transformer layers when facing diverse deepfake generation techniques.

% For the 21 LA dataset, our method achieves 2.46\% EER, showing competitive performance compared to specialized methods like Wav2Vec+AASIST (0.82\%) and OCKD (0.90\%), while substantially outperforming the XLS-R + SLS baseline (3.88\%). On the real-world In-the-Wild benchmark, we achieve 6.87\% EER, representing 22.5\% relative improvement over XLS-R + SLS (8.87\%) and demonstrating strong generalization to diverse recording conditions.

% The consistent performance across all three datasets validates our core hypothesis: hierarchical modeling of inter-layer dependencies, combined with contrastive regularization, effectively captures complementary information across the full representational hierarchy for robust audio deepfake detection.

\begin{table}[t]
\centering
\caption{Ablation study isolating contributions of hierarchical attention and contrastive learning. Bold shows the best.}
\label{tab:ablation}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{21 LA} & \textbf{21 DF} & \textbf{ITW} \\
\midrule
XLS-R + SLS~\cite{zhang2024audio} (Baseline) & 3.88 & 2.09 & 8.87 \\
\midrule
+ Hierarchical Attention & 2.97 & 2.13 & 8.81 \\
+ Hier. Attn + Contrastive Learning & \textbf{2.46} & \textbf{1.93} & \textbf{6.87} \\
\bottomrule
\end{tabular}
\vspace{-2em}
\end{table}


\subsection{Ablation Studies}
To quantify component-level contributions, we incrementally activate HierCon modules beginning from the XLS-R + SLS baseline. Results are summarized in Table ~\ref{tab:ablation}. 
The baseline performs reasonably well in-domain but struggles under distribution shift, particularly in ITW, indicating its limited ability to capture robust generative artifacts beyond fixed layer-level weighting.
Adding hierarchical attention without contrastive learning provides substantial gains on cross-domain datasets: 23.5\% relative improvement on LA (2.97\% vs 3.88\%) and a modest 0.7\% improvement on In-the-Wild (8.81\% vs 8.87\%). However, performance on DF slightly degrades (2.13\% vs 2.09\%), suggesting hierarchical modeling effectively captures inter-layer dependencies for generalization but may overfit to dataset-specific statistical cues when not regularised. This behaviour indicates that while structural modelling strengthens feature abstraction, it requires additional stabilisation to remain resilient across unseen attack paradigms.
The combination of hierarchical attention with margin-based contrastive learning yields the best performance across all datasets, with dramatic improvements on the most challenging generalisation benchmark: 27.7\% relative gain over SLS on In-the-Wild (6.87\% vs 8.87\%). Notably, contrastive learning also alleviates the DF degradation (1.93\% vs 2.13\%), demonstrating its role in enforcing a more transferable representation structure. The gains imply that contrastive regularisation not only strengthens class separation but also prevents the hierarchical mechanism from relying on shallow artefacts inconsistent across spoofing algorithms or recording environments.
These findings validate the dual-objective design. Hierarchical attention captures complementary cues distributed across transformer layers, while contrastive learning encourages more discriminative and less dataset-dependent representations. The consistent improvements across benchmarks suggest that both architectural and training components jointly contribute to stronger generalization, enabling more reliable deepfake detection.
% \noindent\textbf{Hierarchical Attention Alone.} 

% \noindent\textbf{Adding Contrastive Learning.} 


% To understand the individual contributions of hierarchical attention and contrastive learning, we perform systematic ablation experiments. Table~\ref{tab:ablation} presents the results.



% Starting from the SLS baseline (independent layer weighting), we observe distinct patterns across datasets that reveal the complementary nature of our two contributions:

% \textit{Hierarchical Attention Alone.} Adding hierarchical attention without contrastive learning provides substantial gains on cross-domain datasets: 23.5\% relative improvement on LA (2.97\% vs 3.88\%) and modest 0.7\% improvement on In-the-Wild (8.81\% vs 8.87\%). However, performance on DF slightly degrades (2.13\% vs 2.09\%). This pattern suggests hierarchical modeling effectively captures inter-layer dependencies for generalization but can overfit to domain-specific patterns without proper regularization.

% \textit{Adding Contrastive Learning.} The combination of hierarchical attention with margin-based contrastive learning yields the best results across all datasets, with dramatic improvements on the most challenging generalization benchmark: 27.7\% relative gain over SLS on In-the-Wild (6.87\% vs 8.87\%). Notably, contrastive learning also improves DF performance (1.93\% vs 2.13\%), addressing the slight degradation observed with attention alone. 

% These complementary effects validate our dual-objective design: hierarchical attention captures multi-level dependencies across transformer layers, while contrastive regularization prevents the model from overfitting to domain-specific artifacts, ensuring learned features generalize to cross-domain generation methods and recording conditions. The consistent improvements demonstrate that both components are essential for robust real-world deepfake detection.

\subsection{Attention Analysis and Interpretability}
To evaluate whether the learned behavior aligns with the intended design, we analyze attention distributions across the three HierCon stages. As can be seen in Figure~\ref{fig:attention_composite}, temporal attention consistently prioritizes central regions of the signal while down-weighting boundaries, a pattern consistent with established forensic observations that synthetic artifacts are more likely to appear during sustained speech rather than at the onset or trailing edges. The consistency of this behavior across runs suggests that the model is learning stable temporal cues rather than relying on optimization noise or sample-specific alignment bias.
At the intra-group stage, we observe a gradual shift from emphasizing shallow, acoustically driven layers toward deeper, semantically oriented layers as processing transitions across groups. This progression reflects how modern generative systems introduce artifacts at multiple abstraction levels: shallow layers tend to expose subtle spectral distortions or noise shaping irregularities, while deeper layers capture unnatural prosody, rhythm, or linguistic instabilities introduced by synthesis pipelines. Meanwhile, inter-group attention places the highest emphasis on mid-level transformer layers. This suggests that the most discriminative evidence emerges from the interaction zone between purely acoustic and purely semantic representations, aligning with prior findings in multi-layer self-supervised feature fusion research.
Beyond structural inspection, we also observe that attention patterns remain highly consistent across different spoofing mechanisms, including vocoder-based, diffusion-based, and voice conversion models. This convergence implies that the model is not simply memorizing synthesis-specific signatures but instead identifying recurrent artifact structures that persist despite variation in generation pipelines. The stability of these patterns across domains supports the hypothesis that HierCon learns transferable cues grounded in the fundamental properties of synthetic speech.


Taken together, the interpretability analysis indicates that HierCon improves performance not through opaque shortcut features but through structured, coherent use of multi-scale representations aligned with known spoofing signal characteristics. The observed behaviors demonstrate that the model develops reasoning across abstraction levels rather than depending solely on localized artifacts, reinforcing confidence in its resilience and generalization beyond controlled benchmark datasets.


% Figure~\ref{fig:attention_composite} visualizes learned hierarchical attention patterns averaged over 200 DF samples, revealing interpretable detection mechanisms at each stage. Temporal attention (Figure~\ref{fig:temporal_attention}) concentrates on stable middle segments (40-70\% duration), naturally avoiding boundary artifacts and segmentation noise. Intra-group attention (Figure~\ref{fig:intra_attention}) exhibits adaptive layer selection: early groups emphasize shallow layers (Group 1, L0: 0.438) for acoustic artifacts, while late groups favor deep layers (Group 8, L2: 0.572) for semantic cues. Inter-group attention (Figure~\ref{fig:inter_attention}) reveals mid-level dominance, with Group 5 (layers 12-14) receiving highest weight (0.243), validating that intermediate representations balancing acoustic and contextual features are most discriminative. These learned patterns confirm each stage addresses its intended challenge without manual engineering.

\section{Conclusion}
We introduced HierCon, a hierarchical attention framework with contrastive learning for audio deepfake detection. The method addresses limitations of prior SSL-based approaches by modeling inter-layer dependencies and enforcing domain-invariant embedding structure.
Across ASVspoof 2021 and In-the-Wild benchmarks, HierCon achieves state-of-the-art performance, including 1.93\% EER on DF and a 22.5\% relative improvement over XLS-R+SLS~\cite{zhang2024audio} in real-world conditions. Ablation and interpretability analyses confirm that hierarchical attention and contrastive learning contribute complementary benefits, enabling both improved generalization and transparent decision behavior.
Future work will examine cross-model extension, multilingual robustness, and deployment-efficient variants suitable for real-time forensic use.
% We presented a hierarchical layer attention framework with contrastive learning for robust audio deepfake detection. Our approach addresses two critical limitations of existing methods: (1) feature homogenization from treating SSL transformer layers independently, and (2) domain overfitting that degrades generalization to cross-domain generation methods and recording conditions.

% Through explicit modeling of inter-layer dependencies via three-stage attention---temporal, intra-group, and inter-group---our framework dynamically identifies and integrates complementary information across the full representational hierarchy. Margin-based contrastive regularization further enhances robustness by encouraging domain-invariant embeddings that separate real and fake speech across diverse acoustic conditions.

% Extensive experiments demonstrate strong performance on ASVs-poof 2021 DF (1.93\% EER), LA (2.46\% EER), and In-the-Wild (6.87\% EER) datasets, with substantial improvements over the independent layer weighting baseline (36.6\% and 22.5\% relative gains on LA and ITW respectively). Ablation studies confirm that hierarchical attention and contrastive learning provide complementary benefits essential for robust detection. Attention visualization reveals interpretable patterns---temporal focus on stable regions, adaptive layer selection within abstraction levels, and emphasis on intermediate representations---validating our design choice.

% Future work will explore extension to other SSL architectures (e.g., WavLM, HuBERT), investigation of cross-lingual generalization given XLS-R's multilingual pre-training, and development of lightweight variants for real-time deployment scenarios. The interpretability provided by hierarchical attention also opens avenues for explainable AI in audio forensics applications.

\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\end{document}