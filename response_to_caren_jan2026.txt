Dear Caren,

Thank you for the clarifying feedback. The reframing from "interpretable features" to "decision cues" is very helpful and shifts the focus to what actually matters for deepfake detection.

I completely agree that the key evaluation criteria should be:
(i) Causal relevance to classifier predictions
(ii) Consistency of cue usage over time for the same utterance

The temporal instability I observed is indeed more concerning when viewed through this lens—it suggests the model is making decisions based on unstable evidence, which could indicate brittle reasoning.

## Proposed Approach

I plan to focus on directions #3 and #4, specifically in this order:

**Phase 1: Cue Consistency Analysis (diagnostic)**
Before implementing any regularization, I want to understand which SAE features are actually decision-relevant and diagnose the source of instability:

*Step 1: Feature Attribution Analysis*
- Identify causally relevant features using gradient-based attribution (computing ∂logits/∂SAE_activations)
- Rank features by their influence on the classifier's predictions
- Validate top-K features using ablation on a subset to confirm causal impact

*Step 2: Instability Source Analysis*
- Measure temporal stability metrics (Jaccard, lifetime, flipping rate) specifically for high-influence features vs. all features
- Compare instability at window boundaries vs. interior frames to isolate whether hard windowing is the primary cause
- Analyze cross-boundary discontinuities specifically for decision-relevant features

*Step 3: Decision Cue Characterization*
- Introduce a new metric: "Decision cue overlap" - measuring whether the same subset of decision-relevant features consistently activates for the same utterance
- Classify decision features by behavior type:
  - Persistent features (should stay active, instability is problematic)
  - Transient features (respond to brief artifacts, instability may be appropriate)
- Compare cue reuse patterns between genuine vs. spoofed samples to understand whether decision strategies differ

This diagnostic phase will reveal:
1. Whether instability is concentrated in decision-relevant features (indicating decision brittleness)
2. Whether window boundaries are the primary culprit or if instability is inherent
3. Which types of features require stability constraints vs. which should remain flexible

**Phase 2: Decision-Aware Regularization (solution)**
Based on Phase 1 insights, I will implement targeted regularization:

*Approach A: Adaptive Temporal Regularization*
- Weight temporal consistency loss by each feature's causal relevance: L_temporal = Σ_f w_f · ||a_f^t - a_f^(t+1)||^2
- Apply stronger constraints near window boundaries where discontinuities occur
- Differentiate between persistent features (high penalty) and transient features (low penalty)

*Approach B: Cue Consistency Regularization*
- Rather than smoothing individual features, encourage the active decision cue set to remain consistent
- Penalize frequent switching between different subsets of decision features within the same utterance
- This directly addresses the cue reuse problem you highlighted

*Approach C (if justified by Phase 1): Class-Specific Regularization*
- If genuine and spoofed samples show different instability patterns, apply tailored constraints
- For example, if spoof detection shows more cue switching, apply stronger consistency regularization to spoof-specific features

## Experiments for Next Meeting

I propose to prepare the following for our discussion:

1. **Feature attribution analysis**: 
   - Gradient-based attribution scores for all SAE features
   - Identification of top-K decision-relevant features (likely top 10-20%)
   - Ablation validation for top-10 features to confirm causal impact

2. **Instability characterization**:
   - Decision-relevant stability metrics vs. all-feature metrics
   - Boundary vs. interior frame analysis showing where instability occurs
   - Feature behavior classification (persistent, transient, boundary-sensitive)

3. **Cue consistency analysis**: 
   - Decision cue overlap scores for genuine vs. spoofed utterances
   - Visualization of which feature subsets are used for decisions over time
   - Statistical comparison of cue reuse patterns between classes

4. **Mechanistic insights**:
   - Whether the window-based TopK specifically disrupts important decision cues
   - Whether certain types of spoofing attacks trigger more unstable cue usage
   - Feature interaction analysis (do certain features co-activate, suggesting ensemble reasoning?)

These experiments will provide clear evidence for whether decision instability is the core problem and guide the design of principled regularization strategies. The goal is not just to smooth activations, but to ensure the model reasons consistently about the same evidence.

I estimate I can have initial diagnostic results (Phase 1, items 1-3) ready within one week. Please let me know if you'd like me to prioritize any specific aspect or if additional analyses would be helpful before we proceed to the regularization phase.

Best regards,
Nic
