import torch
import torch.nn as nn
import torch.nn.functional as F
import fairseq
from typing import List, Tuple, Optional
import math


class SSLModel(nn.Module):
    """SSL feature extractor wrapper"""
    def __init__(self, cp_path: str, device: torch.device):
        super().__init__()
        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])
        self.model = model[0]
        self.device=device
        self.out_dim = 1024
        return

    def extract_feat(self, input_data):
        if next(self.model.parameters()).device != input_data.device \
           or next(self.model.parameters()).dtype != input_data.dtype:
            self.model.to(input_data.device, dtype=input_data.dtype)
            self.model.train()

        if input_data.ndim == 3:
            input_tmp = input_data[:, :, 0]
        else:
            input_tmp = input_data
            
        result = self.model(input_tmp, mask=False, features_only=True)
        emb = result['x']
        layer_results = result['layer_results']
        return emb, layer_results



class PositionalEncoding(nn.Module):
    """Learnable positional encoding for layer ordering"""
    def __init__(self, num_layers: int, d_model: int):
        super().__init__()
        self.pos_emb = nn.Parameter(torch.randn(1, num_layers, d_model) * 0.02)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, L, D = x.shape
        if L != self.pos_emb.size(1):
            pos_emb = self.pos_emb[:, :L, :]
        else:
            pos_emb = self.pos_emb
        return x + pos_emb


class EfficientAttnPool(nn.Module):
    def __init__(self, in_dim: int, attn_dim: int = 128, dropout: float = 0.1):
        super().__init__()
        self.scale = attn_dim ** -0.5
        self.q = nn.Linear(in_dim, attn_dim)
        self.k = nn.Linear(in_dim, attn_dim)
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        q = self.q(x.mean(dim=1, keepdim=True))  # [B, 1, attn_dim]
        k = self.k(x)  # [B, T, attn_dim]
        
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        scores = scores.squeeze(1)
        
        if mask is not None:
            scores = scores.masked_fill(~mask.bool(), float('-inf'))
        
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        out = torch.sum(attn.unsqueeze(-1) * x, dim=1)
        return out, attn


class AdaptiveGrouping(nn.Module):
    """
    ğŸ”§ ç®€åŒ–ç‰ˆè‡ªé€‚åº”åˆ†ç»„
    - ç§»é™¤æ¸©åº¦ clamp
    - ä½¿ç”¨ Gumbel-Softmax è¿›è¡Œæ›´é”åˆ©çš„åˆ†ç»„
    """
    def __init__(self, num_groups: int, d_model: int):
        super().__init__()
        self.num_groups = num_groups
        # ğŸ”§ ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–ï¼Œç¡®ä¿åˆå§‹åˆ†ç»„æ›´æ¸…æ™°
        centers = torch.empty(num_groups, d_model)
        nn.init.orthogonal_(centers)
        self.group_centers = nn.Parameter(centers)
        # ğŸ”§ æ¸©åº¦åˆå§‹åŒ–ä¸º 0.5ï¼ˆæ›´æ¿€è¿›ï¼‰
        self.temperature = nn.Parameter(torch.tensor(0.5))
    
    def forward(self, layer_emb: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        B, L, D = layer_emb.shape
        
        # Compute similarity
        layer_emb_norm = F.normalize(layer_emb, dim=-1)
        centers_norm = F.normalize(self.group_centers, dim=-1)
        sim = torch.matmul(layer_emb_norm, centers_norm.T)  # [B, L, num_groups]
        
        # ğŸ”§ ç§»é™¤ clampï¼Œå…è®¸æ¸©åº¦è‡ªç”±å­¦ä¹ 
        # ä½¿ç”¨ Gumbel-Softmax è¿›è¡Œæ›´é”åˆ©çš„åˆ†ç»„
        if self.training:
            # Gumbel noise for sharp assignment during training
            gumbel_noise = -torch.log(-torch.log(torch.rand_like(sim) + 1e-8) + 1e-8)
            assignments = F.softmax((sim + gumbel_noise) / self.temperature.abs(), dim=-1)
        else:
            assignments = F.softmax(sim / self.temperature.abs(), dim=-1)
        
        # Weighted aggregation
        grouped = torch.matmul(assignments.transpose(1, 2), layer_emb)
        
        return grouped, assignments


class ResidualAttention(nn.Module):
    """Attention block with layer normalization"""
    def __init__(self, in_dim: int, attn_dim: int = 128, dropout: float = 0.1):
        super().__init__()
        self.norm = nn.LayerNorm(in_dim)
        self.attn = EfficientAttnPool(in_dim, attn_dim, dropout)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        normed = self.norm(x)
        pooled, weights = self.attn(normed)
        return pooled, weights


class SimplifiedTemporalConv(nn.Module):
    """ğŸ”§ ç®€åŒ–ç‰ˆæ—¶åºå·ç§¯ - å•ä¸ªæ·±åº¦å¯åˆ†ç¦»å·ç§¯"""
    def __init__(self, in_dim: int):
        super().__init__()
        # Depthwise separable conv (æ›´é«˜æ•ˆ)
        self.depthwise = nn.Conv1d(in_dim, in_dim, kernel_size=3, padding=1, groups=in_dim)
        self.pointwise = nn.Conv1d(in_dim, in_dim, kernel_size=1)
        self.norm = nn.LayerNorm(in_dim)
        self.activation = nn.GELU()
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """x: [B, T, C]"""
        x_t = x.transpose(1, 2)  # [B, C, T]
        out = self.depthwise(x_t)
        out = self.pointwise(out)
        out = out.transpose(1, 2)  # [B, T, C]
        return self.activation(self.norm(out))


class AdaptiveGroupingHierarchicalModel(nn.Module):
    """
    ğŸ”§ ä¼˜åŒ–åçš„è‡ªé€‚åº”åˆ†ç»„å±‚æ¬¡æ¨¡å‹
    ä¸»è¦æ”¹è¿›:
    1. å‡å°‘åˆ†ç»„æ•° (8 -> 4)
    2. ç®€åŒ–æ—¶åºå¤„ç†
    3. æ·»åŠ  warmup æœºåˆ¶
    4. æ›´å¼ºçš„æ®‹å·®è¿æ¥
    """
    def __init__(self, args, device: torch.device):
        super().__init__()
        self.device = device
        
        # SSL model
        cp_path = getattr(args, 'ssl_checkpoint', 
                         '/root/autodl-tmp/SLSforASVspoof-2021-DF/xlsr2_300m.pt')
        self.ssl_model = SSLModel(cp_path, device)
        self.d_model = self.ssl_model.out_dim
        
        # ğŸ”§ Configuration: å‡å°‘åˆ†ç»„æ•°
        self.num_groups = getattr(args, "num_groups", 4)  # ä» 8 æ”¹ä¸º 4
        self.use_multiscale = getattr(args, "use_multiscale", True)
        
        # ğŸ”§ ç®€åŒ–æ—¶åºå¤„ç†
        if self.use_multiscale:
            self.temporal_conv = SimplifiedTemporalConv(self.d_model)
        
        # Positional encoding
        self.pos_encoding = PositionalEncoding(num_layers=30, d_model=self.d_model)
        
        # Hierarchical attention
        self.temporal_attn = ResidualAttention(self.d_model, attn_dim=256, dropout=0.1)
        
        # ğŸ”§ Adaptive grouping with improved initialization
        self.adaptive_grouping = AdaptiveGrouping(
            num_groups=self.num_groups, 
            d_model=self.d_model
        )
        
        self.intra_attn = ResidualAttention(self.d_model, attn_dim=256, dropout=0.1)
        self.inter_attn = ResidualAttention(self.d_model, attn_dim=256, dropout=0.1)
        
        # ğŸ”§ æ›´ç®€å•çš„ refinement
        self.group_refine = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, self.d_model),
            nn.GELU(),
            nn.Dropout(0.1),
        )
        
        self.utt_refine = nn.Sequential(
            nn.LayerNorm(self.d_model),
            nn.Linear(self.d_model, self.d_model),
            nn.GELU(),
            nn.Dropout(0.1),
        )
        
        # Pre-classifier normalization
        self.pre_classifier_norm = nn.LayerNorm(self.d_model)
        
        # ğŸ”§ Classifier with stronger dropout
        self.classifier = nn.Sequential(
            nn.Linear(self.d_model, 512),
            nn.GELU(),
            nn.Dropout(0.3),  # ä» 0.2 å¢åŠ åˆ° 0.3
            nn.Linear(512, 2),
        )
        
        # Weight initialization
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.trunc_normal_(module.weight, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor, return_attention: bool = False):
        assert x.ndim in [2, 3], f"Expected 2D or 3D input, got {x.ndim}D"
        
        # Extract SSL features
        _, layer_results = self.ssl_model.extract_feat(x.squeeze(-1))
        
        # Convert to tensors
        layer_features = []
        for hidden, _ in layer_results:
            layer_features.append(hidden.transpose(0, 1))  # [T, B, D] -> [B, T, D]
        
        layer_stack = torch.stack(layer_features, dim=1)  # [B, L, T, D]
        B, L, T, D = layer_stack.shape
        
        # ====== Level 1: Temporal Processing ======
        if self.use_multiscale:
            # ğŸ”§ å¤„ç†æ‰€æœ‰å±‚ï¼Œä½†ä½¿ç”¨æ›´é«˜æ•ˆçš„å·ç§¯
            layer_emb_list = []
            for l in range(L):
                # æ—¶åºå»ºæ¨¡
                temporal_feat = self.temporal_conv(layer_stack[:, l])  # [B, T, D]
                # æ± åŒ–
                pooled, _ = self.temporal_attn(temporal_feat)  # [B, D]
                layer_emb_list.append(pooled)
            layer_emb = torch.stack(layer_emb_list, dim=1)  # [B, L, D]
        else:
            # ç›´æ¥æ± åŒ–ï¼ˆæœ€å¿«ï¼‰
            layer_tokens = layer_stack.reshape(B * L, T, D)
            layer_pooled, _ = self.temporal_attn(layer_tokens)
            layer_emb = layer_pooled.reshape(B, L, D)
        
        # Add positional encoding
        layer_emb = self.pos_encoding(layer_emb)  # [B, L, D]
        
        # ====== Level 2: Adaptive Grouping ======
        grouped_emb, group_assignments = self.adaptive_grouping(layer_emb)  # [B, num_groups, D]
        
        # Intra-group attention
        group_vecs = []
        for g_idx in range(self.num_groups):
            g_vec, _ = self.intra_attn(grouped_emb[:, g_idx:g_idx+1, :])
            # ğŸ”§ æ›´å¼ºçš„æ®‹å·®è¿æ¥
            g_vec = g_vec + 0.5 * self.group_refine(g_vec)  # ç¼©æ”¾æ®‹å·®
            group_vecs.append(g_vec)
        
        # ====== Level 3: Inter-group Attention ======
        group_stack = torch.stack(group_vecs, dim=1)  # [B, num_groups, D]
        utt_emb, inter_weights = self.inter_attn(group_stack)
        # ğŸ”§ æ›´å¼ºçš„æ®‹å·®è¿æ¥
        utt_emb = utt_emb + 0.5 * self.utt_refine(utt_emb)
        
        # ====== Classification ======
        utt_emb = self.pre_classifier_norm(utt_emb)
        logits = self.classifier(utt_emb)
        output = F.log_softmax(logits, dim=-1)
        
        if return_attention:
            attention_dict = {
                'group_assignments': group_assignments,
                'inter_weights': inter_weights,
                'num_layers': L,
                'num_groups': self.num_groups,
                'temperature': self.adaptive_grouping.temperature.item(),  # ğŸ”§ ç›‘æ§æ¸©åº¦
            }
            return output, attention_dict
        
        return output








